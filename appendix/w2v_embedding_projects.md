# Word2vec + Word Embeddings (2016-2018)
After the publication of word2vec thesis in 2013 by Google, France and China have projects to develop word2vec method with word embedding application.
The appearance of Poincaré Embeddings in 2017 is remarkable because Poincaré space is more efficient than conventional Euclidean space in term of word2vec-based Word embedding task.
Then, word2vec itself was already recognized as a popular technology applied in word embeddings.

[K.Janod et al, "Réseaux de Neurones pour la Représentation de Contextes Continus des Mots", 2016]

The French thesis mentioned above had its experimental result related to DECODA project, French NLP project for theme recognition for French telephone conversations, and utilized French corpus GigaWord, Wikipedia, extracts from AFP, Le Monde and Le Soir etc.

[X.Yang and et al, "Automatic Construction and Optimization of Sentiment Lexicon based on Word2Vec", 2017]

The Chinese thesis mentioned above had its experimental result related to the automatic construction and optimization of sentiment lexicon "SentiRuc", utilize Sougou Xinwen(搜狗新闻) corpus prosessed by word2vec.

As we know, Word2vec and Word Embeddings originated in different field of study. Word2vec is expected to use 2 types of simple neural network: CBOW and Skip-gram.
However, Word Embedding itself is language model of Natural Language Processing (NLP). Then combination for two of them is a new method mainly studied on the field of Artificial Intelligence.
